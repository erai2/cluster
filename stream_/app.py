import os
from pathlib import Path
import streamlit as st
from parser import parse_file
from normalization.hanja_norm import normalize_hanja
from condition_filter import filter_sentences
from gpt_extractor_v2 import extract_rules

OPENAI_API_KEY = os.environ.get("OPENAI_API_KEY")

if not OPENAI_API_KEY:
    st.error("í™˜ê²½ë³€ìˆ˜ OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    st.stop()

st.title("ğŸ“š ê·œì¹™ ì¶”ì¶œê¸°")

uploaded_file = st.file_uploader("ë¶„ì„í•  íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”", type=["txt", "docx", "pdf"])

if uploaded_file is not None:
    input_path = Path("input") / uploaded_file.name
    with open(input_path, "wb") as f:
        f.write(uploaded_file.getbuffer())

    st.write("íŒŒì¼ ì²˜ë¦¬ ì¤‘...")
    text = parse_file(input_path)
    text = normalize_hanja(text)
    candidate_sents = filter_sentences(text.split("\n"))
    rules = extract_rules(candidate_sents, OPENAI_API_KEY)

    st.success("ì¶”ì¶œ ì™„ë£Œ!")
    st.json(rules)

    output_path = Path("output") / f"{input_path.stem}_rules.json"
    with open(output_path, "w", encoding="utf-8") as f:
        f.write(str(rules))
    st.download_button("ğŸ“¥ ê²°ê³¼ ë‹¤ìš´ë¡œë“œ", data=str(rules), file_name=f"{input_path.stem}_rules.json")
