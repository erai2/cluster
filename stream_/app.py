import streamlit as st
import sqlite3
import pandas as pd
import openai
import json
import tempfile
import os
import numpy as np

from PyPDF2 import PdfReader
import docx
import chardet

# === ì„¤ì • ===
DB_FILE = "terms.db"
openai.api_key = st.secrets["OPENAI_API_KEY"]

# === DB ì´ˆê¸°í™” ===
def init_db():
    conn = sqlite3.connect(DB_FILE)
    c = conn.cursor()
    c.execute("""
        CREATE TABLE IF NOT EXISTS terms (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            category TEXT,
            term TEXT,
            definition TEXT,
            explanation TEXT,
            examples TEXT,
            rules TEXT,
            keywords TEXT,
            source_file TEXT,
            embedding BLOB,
            date_added TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        )
    """)
    conn.commit()
    conn.close()

# === íŒŒì¼ í…ìŠ¤íŠ¸ ì¶”ì¶œ ===
def extract_text(file):
    ext = file.name.split(".")[-1].lower()
    if ext in ["txt", "md"]:
        raw = file.read()
        try:
            encoding = chardet.detect(raw)["encoding"] or "utf-8"
            return raw.decode(encoding)
        except:
            return raw.decode("utf-8", errors="ignore")

    elif ext == "pdf":
        with tempfile.NamedTemporaryFile(delete=False) as tmp:
            tmp.write(file.read())
            tmp_path = tmp.name
        reader = PdfReader(tmp_path)
        text = "\n".join([page.extract_text() for page in reader.pages if page.extract_text()])
        os.remove(tmp_path)
        return text

    elif ext in ["docx", "doc"]:
        with tempfile.NamedTemporaryFile(delete=False) as tmp:
            tmp.write(file.read())
            tmp_path = tmp.name
        doc = docx.Document(tmp_path)
        text = "\n".join([para.text for para in doc.paragraphs])
        os.remove(tmp_path)
        return text

    elif ext == "csv":
        df = pd.read_csv(file)
        return "\n".join(df.astype(str).apply(lambda x: " ".join(x), axis=1))

    else:
        st.error("ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤.")
        return ""

# === AI ë¶„ì„ ===
def extract_terms(text):
    prompt = f"""
    ë‹¤ìŒ ë¬¸ì„œì—ì„œ ì£¼ìš” ìš©ì–´, ì •ì˜, ì„¤ëª…, ì‚¬ë¡€, ê·œì¹™, í‚¤ì›Œë“œë¥¼ JSON ë°°ì—´ë¡œ ì¶œë ¥:
    - category: ì¹´í…Œê³ ë¦¬
    - term: ìš©ì–´ëª…
    - definition: ì§§ì€ ì •ì˜
    - explanation: ê¸´ ì„¤ëª…
    - examples: ì‚¬ë¡€ ë°°ì—´
    - rules: ê·œì¹™ ë°°ì—´
    - keywords: í‚¤ì›Œë“œ ë°°ì—´

    ë¬¸ì„œ:
    {text}
    """
    resp = openai.ChatCompletion.create(
        model="gpt-4o-mini",
        messages=[{"role":"user","content":prompt}],
        temperature=0
    )
    try:
        return json.loads(resp.choices[0].message.content)
    except:
        st.error("AI ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨")
        return []

# === ì„ë² ë”© ìƒì„± ===
def get_embedding(text):
    resp = openai.Embedding.create(
        model="text-embedding-3-small",
        input=text
    )
    return np.array(resp["data"][0]["embedding"], dtype=np.float32)

# === DB ì €ì¥ ===
def save_to_db(data, source_file):
    conn = sqlite3.connect(DB_FILE)
    c = conn.cursor()
    for item in data:
        emb = get_embedding(item.get('definition', '') + " " + item.get('explanation', ''))
        c.execute("""
            INSERT INTO terms (category, term, definition, explanation, examples, rules, keywords, source_file, embedding)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
        """, (
            item.get('category', ''),
            item.get('term', ''),
            item.get('definition', ''),
            item.get('explanation', ''),
            json.dumps(item.get('examples', []), ensure_ascii=False),
            json.dumps(item.get('rules', []), ensure_ascii=False),
            json.dumps(item.get('keywords', []), ensure_ascii=False),
            source_file,
            emb.tobytes()
        ))
    conn.commit()
    conn.close()

# === ì˜ë¯¸ ê¸°ë°˜ ê²€ìƒ‰ ===
def semantic_search(query, top_n=5):
    q_emb = get_embedding(query)
    conn = sqlite3.connect(DB_FILE)
    df = pd.read_sql("SELECT * FROM terms", conn)
    conn.close()

    if df.empty:
        return pd.DataFrame()

    df["embedding"] = df["embedding"].apply(lambda x: np.frombuffer(x, dtype=np.float32))
    df["score"] = df["embedding"].apply(lambda e: np.dot(q_emb, e) / (np.linalg.norm(q_emb) * np.linalg.norm(e)))
    df = df.sort_values("score", ascending=False).head(top_n)
    return df.drop(columns=["embedding"])

# === UI ===
st.set_page_config(page_title="ë¬¸ì„œ AI ë¶„ì„ DB", layout="wide")
st.title("ğŸ“„ ë¬¸ì„œ â†’ AI ìš©ì–´ ë¶„ì„ DB (ì„ë² ë”© ê²€ìƒ‰ í¬í•¨)")

init_db()

tab1, tab2, tab3 = st.tabs(["ğŸ“¥ ì—…ë¡œë“œ/ë¶„ì„", "ğŸ” DB ê²€ìƒ‰/ìˆ˜ì •", "ğŸ¤– ì˜ë¯¸ ê¸°ë°˜ ê²€ìƒ‰"])

with tab1:
    uploaded = st.file_uploader("ë¬¸ì„œ ì—…ë¡œë“œ", type=["txt","md","pdf","docx","doc","csv"])
    if uploaded:
        text = extract_text(uploaded)
        if text:
            st.subheader("ğŸ“Œ ì›ë³¸ í…ìŠ¤íŠ¸ (ì¼ë¶€ ë¯¸ë¦¬ë³´ê¸°)")
            st.text_area("ì›ë³¸", text[:3000], height=200)
            if st.button("AI ë¶„ì„ ì‹¤í–‰"):
                terms = extract_terms(text)
                if terms:
                    save_to_db(terms, uploaded.name)
                    st.success(f"{len(terms)} ê°œì˜ ìš©ì–´ ì €ì¥ ì™„ë£Œ (íŒŒì¼: {uploaded.name})")
                    st.json(terms)

with tab2:
    conn = sqlite3.connect(DB_FILE)
    df = pd.read_sql("SELECT * FROM terms", conn)
    conn.close()
    search = st.text_input("í‚¤ì›Œë“œ ê²€ìƒ‰")
    if search:
        df = df[df.apply(lambda row: search.lower() in str(row).lower(), axis=1)]
    st.dataframe(df, use_container_width=True)

with tab3:
    query = st.text_input("ìì—°ì–´ë¡œ ê²€ìƒ‰ (ì˜ˆ: 'ë¶€ëª¨ì„±ê³¼ ê´€ë ¨ëœ ê·œì¹™')")
    if query:
        results = semantic_search(query)
        if not results.empty:
            st.dataframe(results, use_container_width=True)
        else:
            st.info("ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")